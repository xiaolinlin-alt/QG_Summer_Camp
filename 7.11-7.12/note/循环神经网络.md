# 循环神经网络

* 掌握循环神经网络原理
* 手写RNN前向和反向逻辑

## 序列模型

* 定义

  * 自然语言、音频、视频以及其他序列数据的模型

* 类型

  * 语音识别

  * 情感分类
  * 机器翻译

* 为什么在序列模型使用CNN等神经网络效果不好啊？

  * 序列前后是有很强的关联性的
  * 序列数据的输入和输出长度不固定



## 循环神经网络

RNN是神经网络的一种，RNN将状态在自身网络中循环传递，可以接收随时间序列结构输入

类型：

![a49c4d98b1bfb3addb74fcbfe718e34](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\a49c4d98b1bfb3addb74fcbfe718e34.png)

* 一对一：固定的输入到输出，如图像分类
* 一对多：固定的输入到序列输出，如图像的文字描述
* 多对一：序列输入到输出，如情感分析，分类正面负面情绪
* 多对多：序列输入到序列的输出，如机器翻译
* 同步多对多：同步序列输入到同步输出，如文本生成，视频每一帧的分类



## 基础循环网络介绍

![f978484ddf3a554ea6ada1b5ce28132](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\f978484ddf3a554ea6ada1b5ce28132.png)

* $$
  x_{t}:网络的输入，每一个时刻的输入
  $$

* $$
  o_{t}:网络的输出，每一个时刻的输出
  $$

* $$
  s_{t}:表示每一个隐层的输出
  $$

* 中间的小圆圈代表隐藏层的一个单元
* 所有单元的参数共享，W，U，V

通用公式：

* s0=0

* $$
  st=g1(U_{xt}+W_{xt-1}+B_a)
  $$

* $$
  ot=g2(V_{st}+b_y)
  $$

g1,g2表示激活函数，g1：tanh/relu，g2：sigmoid，softmax，

其中如果将公式展开:

![3b76318f3be5e127818cfd646331946](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\3b76318f3be5e127818cfd646331946.jpg)

## 序列生成案例

通常对于整个序列给一个开始和结束的标志，start，end标志

![300154dc7820c252af25fb1db5f976c](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\300154dc7820c252af25fb1db5f976c.jpg)

## 词的表示

为了能够让整个网络能够理解我们的输入，需要将词用向量进行表示

* 建立一个包含所有序列词的词典包含（开始和标志两个特殊词，以及没有出现过的词等），每个词在词典中有一个唯一的编号
* 任意一个词都可以用一个N维的one-hot向量来表示。其中，N是词典中包含的词的个数

![8dac0e7b0ae013d4482fb98b4bed1a8](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\8dac0e7b0ae013d4482fb98b4bed1a8.jpg)

我们得到了一个高维的，稀疏的向量（稀疏是指绝大部分元素的值都是0）

## 输出的表示-softmax

* RNN模型，每一个时刻的输出是下一个最可能的词，可以用概率表示（ot），总长度为词的总数长度

* 每一个时刻的输出st都是词的长度，接上softmax回归即可

## 矩阵的运算表示

* 假设：m个词，x_t:m长度
* n是可以人为地去设置的

![caf899e7eb1e815f9bb0be47fef0689](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\caf899e7eb1e815f9bb0be47fef0689.jpg)

## 交叉熵损失

总损失的定义：

* 一整个序列（一个句子）作为一个训练实例，总误差就是各个时刻词的误差之和
* ![109f478b1a72e81e0f6f265225b2a66](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\109f478b1a72e81e0f6f265225b2a66.jpg)

## 时序反向传播算法（BPTT）

对于RNN来说有一个时间概念，需要把梯度沿时间通道传播的BP算法

我们的目标是计算误差关于参数U，V和W以及两个偏置bx，by的梯度，然后使用梯度下降学习出好的参数。

由于这三组参数是共享的，我们需要将一个训练实例在每时刻的梯度相加

* 1.要求：每个事件的梯度都计算出来t=0，t=1，t=2，t=3，t=4，然后加起来的梯度，为每次W更新的梯度值

![624cbce7110838a466983002d47112e](D:\Documents\WeChat Files\wxid_br6yhxkgg2gt22\FileStorage\Temp\624cbce7110838a466983002d47112e.jpg)

* 2.求不同参数的导数步骤：

  * 最后一个cell：
    * 计算最后一个时刻的交叉熵损失对于s_t的梯度，记忆交叉熵损失对于s^t,V,by的导数
    * 按照图中的顺序计算

  * 最后一个前面的cell：
    * 第一步：求出当前层损失对于当前隐层状态输出值s^t的梯度+上一层对于s^t的损失
    * 第二步：计算tanh激活函数的导数
    * 第三步：计算U_xt+W_st-1+b_a的对于不同参数的导数

## 梯度消失与梯度爆炸

由于RNN当中也存在链式求导规则，并且其中序列的长度位置。所以

* 如果矩阵中有非常小的值，并且经过矩阵相乘N次之后，梯度值快速地以指数形式收缩，较远的时刻梯度变为0
* 如果矩阵的值非常大，就会出现梯度爆炸



## 实现简单RNN

我们将构建一个模型，传入一个单词或字符来完成一个句子

该模型会输入一个单词，并预测句子中的下一个字符是什么

这个过程不断重复，直到生成我们所需长度的句子

创建单词词典--->准备输入和标签对--->one-hot编码--->定义模型--->训练模型--->评估模型

![image-20250712190223353](C:\Users\linyu\AppData\Roaming\Typora\typora-user-images\image-20250712190223353.png)







