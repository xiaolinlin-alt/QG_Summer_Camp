## 卷积神经网络：

#### 定义：

深度学习中广泛应用于**图像处理**的一种神经网络结构。

#### CNN变种模型：

##### LeNet：

LeNet的主要贡献是提供了**卷积层和池化层**的组合结构，并且是最早使用CNN进行实际问题解决的模型之一。

##### AlexNet：

引入了**ReLU激活函数**，加速了训练过程。使用了**Dropout技术**，减少了**过拟合**。使用了**数据增强和GPU加速**，是大规模训练变得可能。

##### VGG：

网络深度较大，既有16层和19层。采用了**非常规则**的结构，全部使用**3*3**的卷积核。

##### ***ResNet：***

引入了**残差连接**，使得网络可以非常深，**避免了梯度消失**。ResNet可以非常轻松地训练几百层甚至上千层的网络。

![image-20250407204625308](https://khalillu.oss-cn-guangzhou.aliyuncs.com/khalillu/20250407204625402.png)

###### 步骤：

1. 读数据：将图片数据按照想要的格式读取到程序中
2. 载入模型 ：载入已经编写好的神经网络模型的结构
3. 训练模型：使用训练数据来对神经网络模型进行训练，得到一组参数
4. 验证模型：使用验证数据测试该轮训练出来的模型表现如何
5. 保存模型：将训练出来的参数以文件的形式保存到本地，便于测试时读取
6. 测试模型：检验模型的表现

###### ResNet重点：

* 定义H(x) = f(x) + x，有**f(x) = H(x) - x**，其中f(x)即为残差，在更新时残差的变化率大于普通映射，对变化更加敏感

* 新添加的层训练成**恒等映射**（identityfunction）**f(x) = x**

![image-20250410141206100](https://khalillu.oss-cn-guangzhou.aliyuncs.com/khalillu/20250410141206222.png)

我们只需将**图7.6.2**中右图虚线框内上方的加权运算（如仿射）的**权重和偏置参数**设成**0**，那么f(x)即为**恒等映射**。实际中，当理想映射f(x)**极接近**于恒等映射时，残差映射也易于捕捉 恒等映射的细微波动。

如果想改变通道数，就需要引入一个**额外的1×1卷积层**来将输入变换成需要的形状后再做相加运算。







#### 卷积神经网络基本知识：

![img](https://i2.hdslb.com/bfs/note/8899965db90a92f2ab84e4cedc84d8f24e0b4914.jpg@690w_!web-note.webp)

##### **1.基本（以单通道为例进行学习）**

**什么是卷积？**

——把卷积核放在输入上进行滑窗，将当前卷积核覆盖范围内的输入与卷积核相乘，将值进行累加，得到当前位置的输出；（从数学角度：输入和卷积核的内积运算）；

**本质：融合多个像素值的信息输出一个像素值【下采样】**

**卷积层——提取图像的底层特征**

**池化层——防止过拟合，降维**

​					池化的本质就是采样，降维压缩

**全连接层——汇总卷积层和池化层得到的特征**

**padding——防止边缘的特征被忽略掉**

**上采样——又名放大图像、图像插值**

{主要目的：放大原图像，从而可以显示在更高分辨率的显示设备上；常用方法：双线性插值（在原有图像像素的基础上在像素点之间采用合适的插值算法 [基于边缘的插值、基于区域的图像插值] 插入新的元素，详细介绍的文章如下：“http://t.csdnimg.cn/dYbFb”）；反卷积；反池化}

反卷积（转置卷积）：将下图中的卷积看成是输入通过卷积核的透视，则反卷积就可以看成是输出通过卷积的透视；

![img](https://i2.hdslb.com/bfs/note/5ff638321f94eb93d541b211bd140a64e91cf3b7.png@690w_!web-note.webp)

通过反卷积得到的特征图与卷积输入的特征图像的大小并不相同——>卷积和反卷积并不是完全对等的可逆操作——>反卷积只能恢复尺寸，并不能恢复数值

此时的padding是指内部的空隙

**下采样——又名降采样、缩小图像**

{主要目的：使得图像符合显示区域的大小；生成对应图像的缩略图}

原理：图像I的尺寸为M*N，对其进行s倍的下采样，即得到（M/s）*（N/s）尺寸的分辨率图像，注意：s要是M和N的公约数；如果是矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值或者最大值（即池化操作）

**感受野——**特征图上的某个点能看到的输入图像的区域；找感受野——找输出图像对应到输入中的区域；计算感受野【计算第i层输出结果的感受野，从RF(i)计算到RF(1):**RF(i)=S(i)(RF(i+1)-1)+K(i)**】;使用连续的小卷积核替换单个大卷积核，可以有效**降低网络训练的参数量**

![img](https://i2.hdslb.com/bfs/note/06158d491c29867d932e867d9372a67e02e6e3c1.jpg@690w_!web-note.webp)

![img](https://i2.hdslb.com/bfs/note/c79cecfdec5a1e30c77c44638ff73add64869bef.jpg@690w_!web-note.webp)

每一个卷积核都对应着一个feature map（输出）

**有多少个卷积核就有多少个feature map**

**Dilation——棋盘；表示卷积核不再是连续的一片，而是跨像素的感受野——可以获取更加有效的信息**

##### **2.多通道图片的卷积**

![img](https://i2.hdslb.com/bfs/note/6d614e38f3e3cd11629f2b8a9e5bbdb3950ff841.jpg@690w_!web-note.webp)

红的权重跟红色通道的input进行卷积，蓝的权重跟蓝色通道的input进行卷积，绿的权重跟绿色通道的input进行卷积；相当于一个三阶魔方在input进行滑动，这个三阶魔方的所有的卷积操作乘积作为最终的feature map

![img](https://i2.hdslb.com/bfs/note/264e087e1b8f137ff128f769190c363861fc3ced.jpg@690w_!web-note.webp)

![img](https://i2.hdslb.com/bfs/note/c10426dc065f1ec14dbb586930fd7f0886bd6452.jpg@690w_!web-note.webp)

{最左边的是原始图像（输入），第二列是第一个卷积核，进行卷积过后生成最右边上面的feature map，同理第三列的卷积核对最左边的input进行卷积过后生成最右边第二个的feature map}

![img](https://i2.hdslb.com/bfs/note/953debd208968844574771d14b0f4f1069fa12bb.jpg@690w_!web-note.webp)

卷积的目的——提取图像上的特征

![img](https://i2.hdslb.com/bfs/note/600a7df67c69d397ecebec56a62c3d9be1f42532.jpg@690w_!web-note.webp)

卷积核是通过人工智能、机器学习、梯度下降等方法自己找到的，并非人工设置

##### **3.池化**

![img](https://i2.hdslb.com/bfs/note/23d307f7b27b27dd4657804f526cf9a9355d24fc.jpg@690w_!web-note.webp)

每一个大框只选择一个作为其代表值，最大池化就是选择最大值，平均池化就是选择平均值

池化又叫做下采样，相当于在大框中进行采样并选择一个值来代表它

本质——大的变成小的；模糊——防止过拟合

![img](https://i2.hdslb.com/bfs/note/6184ffc77f948c44d925ea03b85037b4bdba6fc0.jpg@690w_!web-note.webp)

作用三：为卷积神经网络带来平移不变性

![img](https://i2.hdslb.com/bfs/note/a9ce6590f7630040f81b6bb78659d4c0e2562333.jpg@690w_!web-note.webp)

把池化层拉平成一个常向量，最后喂给全连接神经网络；全连接神经网络：每一层和上一层的所有神经元相连，密集连接

![img](https://i2.hdslb.com/bfs/note/41875a96687a0fe4e60515678bffc0741c076a9c.jpg@690w_!web-note.webp)

##### **4.卷积神经网络的结构——各层作用**

![img](https://i2.hdslb.com/bfs/note/1c8d6cc486be6345b2de6a0f9a5bbc142dc976b1.jpg@690w_!web-note.webp)

如上图例子，在卷积眼睛的时候，碰到眼睛就会在feature map显示100，否则就会变成0

——>用卷积核可以对图像的底层特征进行抽取，神经网络最后学习到的就是如何布置卷积核，相当于机器学习学出来时用哪个卷积核更加好

![img](https://i2.hdslb.com/bfs/note/086b2dc1417b9b501ffeb4814f12f2f14de7aaf8.jpg@690w_!web-note.webp)

![img](https://i2.hdslb.com/bfs/note/583c5ef6dfd5ad9cef10793131c4be796f916ead.jpg@690w_!web-note.webp)

从上图中可以得出：池化可以带来卷积神经网络的**平移不变性**

![img](https://i2.hdslb.com/bfs/note/ef2fcbea0e9575132b6d514a3019ad606f1e95b9.jpg@690w_!web-note.webp)

https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks-e3f054dd5daa

##### **5.结构解析**

![img](https://i2.hdslb.com/bfs/note/165ddffea1ea4aad54e7b641b275b55fea73c904.jpg@690w_!web-note.webp)

其实本质也是函数

![img](https://i2.hdslb.com/bfs/note/65baa202bda1f2c06753db603ecf28e352f86360.jpg@690w_!web-note.webp)

**CNN三大结构特性**

**（1）局部连接——感受野**

​		局部连接能够大大减少网络的参数。让每个神经元只与输入数据的一个局部区域连接，该连接的空间大小叫做神经元的感受野，它的尺寸是一个超参数，其实就是滤波器的空间尺寸。

**（2）权值共享——卷积**

​		 在卷积层中使用参数共享是用来控制参数的数量。每个滤波器与上一层局部相连，同时每个滤波器的所有局部连接都使用同样的参数——>大大减少网络的参数

**（3）空间或者时间上的下采样——池化pooling**

​		作用：逐渐降低数据的空间尺寸，减少网络中参数的数量，使得计算资源耗费变少，有效控制过拟合

![img](https://i2.hdslb.com/bfs/note/c75e6f97ccb9295fdd44efc33dd8ab7fb69cb334.jpg@690w_!web-note.webp)