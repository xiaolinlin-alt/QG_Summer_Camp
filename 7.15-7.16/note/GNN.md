# GNN

图神经网络，是一种用于处理图形数据的深度学习技术。

图数据是由**节点（Node）**和**边（Edge）**组成的数据，最简单的方式是使用邻接矩阵来表示图形结构，从而捕捉图形中的节点和边的相关性

图数据的信息包含3个层面，分别是节点信息（***V）、边信息（***E**）、图整体（**U）信息，它们通常是用向量来表示。**而图神经网络就是通过学习数据从而得到3个层面向量的最优表示**。

## 对于图数据而言有怎样的任务？

●*图层面的任务*（分类/回归）

例：分子是天然的图，原子是节点，化学键是边。现在要做一个分类，有一个苯环的分子分一类，两个苯环的分子分一类。

**这是图分类任务**。

●*边层面的任务*（分类/回归）

例：UFO拳击赛上，首先通过语义分割把台上的人和环境分离开来。赛场上的人都是节点，现在要做一个预测，预测的是这些人之间的关系，是对抗关系？还是观众watch的关系？还是裁判watch的关系？**这是边分类任务。**

●*节点层面的任务*（分类/回归）

例：假设一个跆拳道俱乐部里有A、B两个教练，所有的会员都是节点。有一天A、B两个跆拳道教练决裂，那么各个学员是愿意和A在一个阵营还是愿意和B在一个阵营？**这是节点分类任务。**

## **图神经网络是如何工作的？**

![image-20250714092948836](C:\Users\linyu\AppData\Roaming\Typora\typora-user-images\image-20250714092948836.png)

input Graph---GNN blocks---Transformed Graph---

Classification layer---Prediction

## 类别

图卷积网络GCN：

领域聚合更新节点

图注意力网络

图自编码器

图生成网络

图时空网络



节点分类任务

网络架构设计原理

为什么这样子设计模型：

浅层GCN结构：两层GCNConv

为什么：更深的容易过度平滑，也就是节点的嵌入变得难以区分，影响分类效果

所以使用两层GCN卷积层采用16个隐藏单元是一个比较合理的做法，有很好的表达能力又避免了过度平滑的问题

隐藏层的维度选择：这个隐藏层之所以设置成16其实是一个信息瓶颈的设计，它既能保证很好的效果，又可以节省资源，同时也符合基准数据集的标准配置



接着是激活函数的非线性部分

我们使用relu激活函数：

好处：缓解梯度消失的问题，因为这个激活函数的导数要么是0，要么是1

它能产生稀疏的表示，过滤掉不相关的邻居信息

relu可能导致死亡神经元的问题，但是在浅层GCN中，这个问题不严重

返回值：是在对数空间中计算概率

正则化优化方面，选择了Adam的优化器，并设置了权重衰减（防止模型过拟合，尤其是在模型中相连节点有相似标签的，太小不够正则化，太大又会限制模型的表达能力）



半监督学习方法

接下来是模型训练

我们使用整个图的训练

利用图的结构来传播信息

200个训练轮次之后计算准确率

模型收敛，避免过拟合

这个实现基于大量图学习的稳定结果，特别适合半监督的节点分类任务

























