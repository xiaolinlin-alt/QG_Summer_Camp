{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "动手实现LLaMA2大模型",
   "id": "3a292aa34ab2145f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:47:55.739943Z",
     "start_time": "2025-08-02T07:47:55.729307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ],
   "id": "d1c25d0ff94d1ef6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-02T07:35:58.734395Z",
     "start_time": "2025-08-02T07:35:58.722577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import PretrainedConfig#通过继承这个类来方便使用transformers库中的一些功能，也方便后续导入Hugging Face功能\n",
    "\n",
    "class ModelConfig(PretrainedConfig):\n",
    "    model_type=\"Tiny-K\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim:int=768,#模型维度\n",
    "        n_layers:int=12,#Tranformer的层数\n",
    "        n_heads:int=16,#注意力机制的头数\n",
    "        n_kv_heads:int=8,#键值头的数量\n",
    "        vocab_size:int=6144,#词汇表的大小\n",
    "        hidden_dim:int=None,#隐藏层的维度\n",
    "        multiple_of:int=64,#模型维度必须为64的倍数\n",
    "        norm_eps:float=1e-5,#归一化层的epsilon参数\n",
    "        max_seq_len:int=512,#最大序列长度\n",
    "        dropout:float=0.0,#dropout的概率\n",
    "        flash_attn:bool=True,#是否使用Flash Attention\n",
    "        **kwargs,):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)\n",
    "#后面会根据这些超参数来构建我们的模型"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建RMSNorm",
   "id": "542a06f5fe3f9e91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:50:00.352420Z",
     "start_time": "2025-08-02T07:50:00.348010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,dim:int,eps:float):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.weight=nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x*self.weight/(torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps))"
   ],
   "id": "1f5168b242e3c383",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:50:02.282509Z",
     "start_time": "2025-08-02T07:50:02.275158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args=ModelConfig()\n",
    "norm=RMSNorm(args.dim,args.norm_eps)"
   ],
   "id": "f2349e5a5525461f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建LLaMA2Attention，GQA来构建，分组查询注意力机制",
   "id": "9e06c84bb98810c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先，在LLaMA2模型中，我们需要将键和值的维度扩展到和查询的维度一样，这样才能进行注意力机制，也就是要实现repeat_kv",
   "id": "3dd44afe57dc22a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:50:04.627401Z",
     "start_time": "2025-08-02T07:50:04.622325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def repeat_kv(x:torch.Tensor,n_rep:int)->torch.Tensor:\n",
    "    #获得输入张量的形状：批次大小、序列长度、键/值对头的数量、每个头的维度\n",
    "    bs,slen,n_kv_heads,head_dim=x.shape\n",
    "    if n_rep==1:\n",
    "        return x\n",
    "    return (x[:,:,:,None,:].expand(bs,slen,n_kv_heads,n_rep,head_dim).reshape(bs,slen,n_kv_heads*n_rep,head_dim))"
   ],
   "id": "afaec80569f2eb14",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "旋转嵌入，可以为注意力机制提供更强的上下文信息",
   "id": "2d4e2a548dfcda4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:50:06.607050Z",
     "start_time": "2025-08-02T07:50:06.603682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def precompute_freqs_cis(dim:int,end:int,theta:float=10000.0):\n",
    "    #torch.arange(0,dim,2)[:(dim//2)].float()生成了一个从0开始，步长为2的序列，长度为dim的一半\n",
    "    freqs=1.0/(theta**(torch.arange(0,dim,2)[:(dim//2)].float()/dim))\n",
    "    #生成一个从0到end的序列，长度为end\n",
    "    t=torch.arange(end,device=freqs.device)\n",
    "    #计算外积，得到一个二维矩阵，每一行是t的元素乘以freps的元素\n",
    "    freqs=torch.outer(t,freqs).float()\n",
    "    #计算频率的余弦值，得到实部\n",
    "    freqs_cos=torch.cos(freqs)\n",
    "    #计算频率的正弦值，得到虚部\n",
    "    freqs_sin=torch.sin(freqs)\n",
    "    return freqs_cos,freqs_sin"
   ],
   "id": "5817b2c51aa46760",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "调整张量的形状,使其在进行广播操作的时候与x的维度对其，从而能进行正确的张量计算",
   "id": "4789dd214c90c1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:54:29.443567Z",
     "start_time": "2025-08-02T07:54:29.432291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reshape_for_broadcast(freqs_cis:torch.Tensor,x:torch.Tensor):\n",
    "    #获取x的维度\n",
    "    ndim=x.ndim\n",
    "    #断言，确保1在x的维度范围内\n",
    "    assert 0<=1<ndim\n",
    "    #断言，确保freqs_cis的形状与x的第二维和最后一维相同\n",
    "    assert freqs_cis.shape==(x.shape[1],x.shape[-1]//2), \\\n",
    "        f\"Shape mismatch: {freqs_cis.shape} vs ({x.shape[1]}, {x.shape[-1]//2})\"\n",
    "    shape=list(x.shape)\n",
    "    shape[0]=1\n",
    "    shape[2]=1\n",
    "    return freqs_cis.view(shape[0],shape[1],shape[2],shape[3]//2)"
   ],
   "id": "400ef65ba94ecf26",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:54:30.925536Z",
     "start_time": "2025-08-02T07:54:30.920310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_rotary_emb(\n",
    "        xq:torch.Tensor,\n",
    "        xk:torch.Tensor,\n",
    "        freqs_cos:torch.Tensor,\n",
    "        freqs_sin:torch.Tensor,\n",
    ")->Tuple[torch.Tensor,torch.Tensor]:\n",
    "    #将查询的和键的张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r,xq_i=xq.float().reshape(xq.shape[:-1]+(-1,2)).unbind(-1)\n",
    "    xk_r,xk_i=xk.float().reshape(xk.shape[:-1]+(-1,2)).unbind(-1)\n",
    "    #计算查询和键的旋转嵌入\n",
    "    freqs_cos,freqs_sin=reshape_for_broadcast(freqs_cos,xq),reshape_for_broadcast(freqs_sin,xq)\n",
    "    #应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r=xq_r*freqs_cos-xq_i*freqs_sin\n",
    "    xq_out_i=xq_r*freqs_sin+xq_i*freqs_cos\n",
    "    xk_out_r=xk_r*freqs_cos-xk_i*freqs_sin\n",
    "    xk_out_i=xk_r*freqs_sin+xk_i*freqs_cos\n",
    "    #将实部和虚部重新组合成复数张量，并重塑为原始形状\n",
    "    xq_out=torch.stack([xq_out_r,xq_out_i],dim=-1).flatten(3)\n",
    "    xk_out=torch.stack([xk_out_r,xk_out_i],dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq),xk_out.type_as(xk)"
   ],
   "id": "f94b33486f4cab33",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "上面完成了旋转嵌入的实现，现在可以来构建Attention模块了",
   "id": "4684002086e6cd3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:50:13.033629Z",
     "start_time": "2025-08-02T07:50:13.021937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,args:ModelConfig):\n",
    "        super().__init__()\n",
    "        #根据是否指定n_kv_heads，确定用于键和值的头的数量\n",
    "        self.n_kv_heads=args.n_kv_head if args.n_kv_heads is None else args.n_kv_heads\n",
    "        #确保总头数可以被键值头数整除\n",
    "        assert args.n_heads%self.n_kv_heads==0\n",
    "        #模型并行处理大小，默认为1\n",
    "        model_parallel_size=1\n",
    "        #本地计算头数\n",
    "        self.n_local_heads=args.n_heads//model_parallel_size\n",
    "        #本地键值头数\n",
    "        self.n_local_kv_heads=self.n_kv_heads//model_parallel_size\n",
    "        #重复次数\n",
    "        self.n_rep=self.n_local_heads//self.n_local_kv_heads\n",
    "        #每个头的维度\n",
    "        self.head_dim=args.dim//args.n_heads\n",
    "        #定义权重矩阵\n",
    "        self.wq=nn.Linear(args.dim,args.n_heads*self.head_dim,bias=False)\n",
    "        self.wk=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
    "        self.wv=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
    "        self.wo=nn.Linear(args.n_heads*self.head_dim,args.dim,bias=False)\n",
    "        #定义dropout层\n",
    "        self.attn_dropout=nn.Dropout(args.dropout)\n",
    "        self.resid_dropout=nn.Dropout(args.dropout)\n",
    "        self.dropout=args.dropout\n",
    "\n",
    "        self.flash=hasattr(torch.nn.functional,'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"Warning: using the slow attention implementation. Install PyTorch nightly to get the fast one.\")\n",
    "\n",
    "            mask=torch.full((1,1,args.max_seq_len,args.max_seq_len),float('-inf'))\n",
    "            mask=torch.triu(mask,diagonal=1)\n",
    "            self.register_buffer('mask',mask)\n",
    "\n",
    "    def forward(self,x:torch.Tensor,freqs_cos:torch.Tensor,freqs_sin:torch.Tensor):\n",
    "        baz,seqlen,_=x.shape\n",
    "        xq,xk,xv=self.wq(x),self.wk(x),self.wv(x)\n",
    "        xq=xq.view(baz,seqlen,self.n_local_heads,self.head_dim)\n",
    "        xk=xk.view(baz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "        xv=xv.view(baz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "        xq,xk=apply_rotary_emb(xq,xk,freqs_cos,freqs_sin)\n",
    "        xk=repeat_kv(xk,self.n_rep)\n",
    "        xv=repeat_kv(xv,self.n_rep)\n",
    "\n",
    "        xq=xq.transpose(1,2)\n",
    "        xk=xk.transpose(1,2)\n",
    "        xv=xv.transpose(1,2)\n",
    "\n",
    "        if self.flash:\n",
    "            output=torch.nn.functional.scaled_dot_product_attention(xq,xk,xv,attn_mask=None,dropout_p=self.dropout if self.training else 0.0,is_causal=True)\n",
    "        else:\n",
    "            scores=torch.matmul(xq,xk.transpose(2,3))/math.sqrt(self.head_dim)\n",
    "            assert hasattr(self,\"mask\")\n",
    "            scores=scores+self.mask[:,:,:seq_len,:seq_len]\n",
    "            scores=F.softmax(scores.float(),dim=-1).type_as(xq)\n",
    "            scores=self.attn_dropout(scores)\n",
    "            output=torch.matmul(scores,xv)\n",
    "        output=output.transpose(1,2).contiguous().view(baz,seqlen,-1)\n",
    "        output=self.wo(output)\n",
    "        output=self.resid_dropout(output)\n",
    "        return output"
   ],
   "id": "8e9c02484b94db75",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:54:36.834015Z",
     "start_time": "2025-08-02T07:54:36.766093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention_model=Attention(ModelConfig())\n",
    "batch_size=1\n",
    "seq_len=50\n",
    "dim=args.dim\n",
    "x=torch.rand(batch_size,seq_len,dim)\n",
    "freqs_cos,freqs_sin=precompute_freqs_cis(dim//args.n_heads,seq_len)\n",
    "output=attention_model(x,freqs_cos,freqs_sin)\n",
    "print(output.shape)"
   ],
   "id": "a286e40e867c5321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:36:34.854447Z",
     "start_time": "2025-08-02T07:36:34.850179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dim:int,hidden_dim:int,multilpe_of:int,dropout:float):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim=dim*4\n",
    "            hidden_dim=int(2*hidden_dim/3)\n",
    "            hidden_dim=multilpe_of*((hidden_dim+multilpe_of-1)//multilpe_of)\n",
    "\n",
    "        self.w1=nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.w2=nn.Linear(hidden_dim,dim,bias=False)\n",
    "        self.w3=nn.Linear(dim,hidden_dim,bias=False)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.w2(F.gelu(self.w1(x))+self.w3(x)))"
   ],
   "id": "a5dd28745dc6c9a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T07:36:36.921526Z",
     "start_time": "2025-08-02T07:36:36.871342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlp=MLP(args.dim,args.hidden_dim,args.multiple_of,args.dropout)\n",
    "x=torch.randn(1,50,args.dim)\n",
    "output=mlp(x)\n",
    "print(output.shape)"
   ],
   "id": "e3d509648933deb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建LlaMA2的Decoder Layer,把我们的Attention模块和MLP模块组合在一起，实现一个完整的transformer模块\n",
   "id": "807eb794c4b0548d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T08:05:27.660831Z",
     "start_time": "2025-08-02T08:05:27.656006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,layer_id:int,args:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads=args.n_heads\n",
    "        self.dim=args.dim\n",
    "        self.head_dim=self.dim//self.n_heads\n",
    "        self.attention=Attention(args)\n",
    "        self.feed_forward=MLP(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            multilpe_of=args.multiple_of,\n",
    "            dropout=args.dropout\n",
    "        )\n",
    "        self.layer_id=layer_id\n",
    "        self.attention_norm=RMSNorm(args.dim,eps=args.norm_eps)\n",
    "        self.ffn_norm=RMSNorm(args.dim,eps=args.norm_eps)\n",
    "    def forward(self,x,freqs_cos,freqs_sin):\n",
    "        h=x+self.attention.forward(self.attention_norm(x),freqs_cos,freqs_sin)\n",
    "        out=h+self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ],
   "id": "52b1fcc107b8c560",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T08:05:30.937500Z",
     "start_time": "2025-08-02T08:05:30.878785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder_layer=DecoderLayer(0,args)\n",
    "dim=args.dim\n",
    "seq_len=50\n",
    "x=torch.randn(1,seq_len,dim)\n",
    "freqs_cos,freqs_sin=precompute_freqs_cis(dim//args.n_heads,seq_len)\n",
    "out=decoder_layer(x,freqs_cos,freqs_sin)\n",
    "print(out.shape)"
   ],
   "id": "9f1e60b041bfa82a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:17:40.163102Z",
     "start_time": "2025-08-02T12:17:40.152389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    config_class=ModelConfig\n",
    "    last_loss:Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self,args: ModelConfig=None):\n",
    "        super().__init__()\n",
    "        self.args=args\n",
    "        self.dim=args.dim\n",
    "        self.vocab_size=args.vocab_size\n",
    "        self.n_layers=args.n_layers\n",
    "        self.tok_embedding=nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.dropout=nn.Dropout(args.dropout)\n",
    "        self.layers=torch.nn.ModuleList()\n",
    "        for layer_id in range(self.n_layers):\n",
    "            self.layers.append(DecoderLayer(layer_id,args))\n",
    "        self.norm=RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        self.output=nn.Linear(self.dim, self.vocab_size, bias=False)\n",
    "        self.tok_embedding.weight=self.output.weight\n",
    "        freqs_cos,freqs_sin=precompute_freqs_cis(\n",
    "            self.dim//args.n_heads,\n",
    "            args.max_seq_len\n",
    "        )\n",
    "        self.register_buffer(\"freqs_cos\",freqs_cos,persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\",freqs_sin,persistent=False)\n",
    "        self.apply(self._init_weights)\n",
    "        for pn,p in self.named_parameters():\n",
    "            if pn.endswith(\"w3.weight\") or pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p,mean=0.0,std=0.02/math.sqrt(2*args.n_layers)\n",
    "                )\n",
    "        self.last_loss=None\n",
    "        self.OUT=CausalLMOutputWithPast()\n",
    "        self._no_split_modules=[name for name, _ in self.named_modules()]\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        if isinstance(module,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None,  **keyargs):\n",
    "        output={}\n",
    "        if \"input_ids\" in keyargs:\n",
    "            tokens=keyargs[\"input_ids\"]\n",
    "        if \"attention_mask\" in keyargs:\n",
    "            targets=keyargs[\"attention_mask\"]\n",
    "        _bsz,_seq_len=tokens.shape\n",
    "        h=self.tok_embedding(tokens)\n",
    "        h=self.dropout(h)\n",
    "        freqs_cos=self.freqs_cos[:seq_len]\n",
    "        freqs_sin=self.freqs_sin[:seq_len]\n",
    "        for layer in self.layers:\n",
    "            h=layer(h,freqs_cos,freqs_sin)\n",
    "        h=self.norm(h)\n",
    "        if targets is not None:\n",
    "            logits=self\n",
    "        if \"input_ids\" in keyargs:\n",
    "            tokens=keyargs[\"input_ids\"]\n",
    "        if \"attention_mask\" in keyargs:\n",
    "            targets=keyargs[\"attention_mask\"]\n",
    "        _bsz,_seq_len=tokens.shape\n",
    "        h=self.tok_embedding(tokens)\n",
    "        h=self.dropout(h)\n",
    "        freqs_cos=self.freqs_cos[:seq_len]\n",
    "        freqs_sin=self.freqs_sin[:seq_len]\n",
    "        for layer in self.layers:\n",
    "            h=layer(h,freqs_cos,freqs_sin)\n",
    "        h=self.norm(h)\n",
    "        if targets is not None:\n",
    "            logits=self.output(h)\n",
    "            self.last_loss=F.cross_entropy(logits.view(-1,self.vocab_size),targets.view(-1),ignore_index=0,reduction=None)\n",
    "        else:\n",
    "            logits=self.output(h)\n",
    "            self.last_loss=None\n",
    "        self.OUT.__setitem__(\"logits\",logits)\n",
    "        self.OUT.__setitem__(\"loss\",self.last_loss)\n",
    "        return self.OUT\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0,top_k=None):\n",
    "        index=idx.shape[1]\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, self.args.max_seq_len:]  #\n",
    "            logits=self(idx_cond).logits\n",
    "            logits=logits[:, -1, :]\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            probs=F.softmax(logits/temperature,dim=-1)\n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "            if idx_next==stop_id:\n",
    "                break\n",
    "            idx=torch.cat((idx,idx_next),dim=-1)\n",
    "        return idx[:,index:]"
   ],
   "id": "8c99aed19fa01c5b",
   "outputs": [],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
